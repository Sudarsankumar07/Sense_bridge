# ğŸŒ‰ SenseBridge - Accessibility Mobile Application

**SenseBridge** is an offline-first, voice-controlled accessibility mobile application built with React Native. It provides three specialized modes designed to assist people with visual, hearing, or speech disabilities through on-device AI processing.

---

## ğŸ“– About the Project

### What is SenseBridge?

SenseBridge is a **zero-cost, privacy-first accessibility app** that runs entirely on your phone without requiring an internet connection. It uses advanced AI models for real-time assistance while keeping all processing local to your device.

### Who is it for?

- **Blind/Visually Impaired Users** - Navigate safely with obstacle detection and currency recognition
- **Deaf/Hard of Hearing Users** - Understand speech through visual sign language animations
- **People who use Sign Language** - Communicate through voice by signing to the camera

### Key Principles

- ğŸ”’ **Privacy First** - All processing happens on your device, no data leaves your phone
- ğŸŒ **Offline Capable** - Works without internet connection
- ğŸ—£ï¸ **Voice-Driven** - Accessible through voice commands for hands-free operation
- â™¿ **Accessibility Focused** - Large buttons, haptic feedback, screen reader compatible
- ğŸ’° **Zero Cost** - Built with free, open-source technologies

---

## ğŸ¯ Features by Mode

### ğŸ¦¯ Blind Mode - Visual Assistance

**Real-time Obstacle Detection:**
- Detects people, stairs, and obstacles in your path
- Voice alerts: "Person ahead", "Step detected", "Obstacle ahead"
- Distance estimation based on object size
- Haptic vibration warnings

**Indian Currency Recognition:**
- Identifies Indian rupee notes and coins
- Speaks denomination: "Ten rupees", "Five hundred rupees"
- Multi-frame verification for accuracy
- Works in various lighting conditions

**How it works:**
1. App uses rear camera to continuously scan environment
2. YOLOv5 AI model detects objects
3. Decision engine filters and throttles alerts
4. Voice announcements guide you safely

---

### ğŸ‘‹ Sign Mode - Sign Language to Voice

**Hand Sign Recognition:**
- Recognizes common sign language gestures
- Converts signs to text on screen
- Speaks recognized words using text-to-speech
- Builds phrases from multiple signs

**Phrase Building:**
- Collects multiple signs into sentences
- Auto-speaks complete phrases after 3 seconds
- Clear phrase button to start over
- Real-time confidence display

**How it works:**
1. Front camera captures your hand gestures
2. MediaPipe extracts hand landmarks (21 points)
3. TFLite classifier identifies the sign
4. Text appears on screen and is spoken aloud

---

### ğŸ§ Deaf Mode - Speech to Sign Language

**Speech Recognition:**
- Converts spoken words to text
- Displays transcript with timestamps
- Fully offline speech recognition

**3D Avatar Animation:**
- Animated sign language avatar
- Performs signs for recognized words
- Fingerspelling for unknown words
- Synchronized subtitles

**How it works:**
1. Microphone captures speech
2. Vosk AI recognizes words offline
3. Unity avatar performs corresponding sign animations
4. Text transcript shows what was said

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           React Native UI Layer             â”‚
â”‚   (Voice-controlled, Accessible Design)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Core Services Layer                â”‚
â”‚  - Voice Engine (TTS + STT)                 â”‚
â”‚  - Decision Engine (Alert Logic)            â”‚
â”‚  - Storage Service (SQLite + AsyncStorage)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Native Modules Layer (Android)       â”‚
â”‚  - TFLiteModule (AI Inference)              â”‚
â”‚  - VoskModule (Offline Speech Recognition)  â”‚
â”‚  - MediaPipeModule (Hand Tracking)          â”‚
â”‚  - UnityBridge (3D Avatar Rendering)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AI Models (On-Device)               â”‚
â”‚  - YOLOv5 Nano (Obstacle Detection)         â”‚
â”‚  - YOLOv5 Nano (Currency Recognition)       â”‚
â”‚  - CNN Classifier (Sign Recognition)        â”‚
â”‚  - Vosk Small English Model                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ How to Run

### Prerequisites

Before you begin, ensure you have:

1. **Node.js** (version 18 or higher)
   ```bash
   node --version
   ```

2. **Android Phone** with:
   - USB cable
   - Developer Mode enabled
   - USB Debugging enabled

3. **Android Studio** (optional, only for SDK tools)

---

### Step-by-Step Installation

#### 1ï¸âƒ£ Clone and Install Dependencies

```bash
# Navigate to project directory
cd "c:\Users\sudarsan kumar\OneDrive\Desktop\Sense_bridge\SenseBridge"

# Install all dependencies
npm install
```

#### 2ï¸âƒ£ Enable Developer Mode on Your Phone

**For Android:**
1. Go to **Settings** â†’ **About Phone**
2. Tap **Build Number** 7 times (you'll see "You are now a developer!")
3. Go back to **Settings** â†’ **Developer Options**
4. Enable **USB Debugging**

#### 3ï¸âƒ£ Connect Your Phone

1. Connect your Android phone to computer via USB cable
2. On phone, allow USB debugging when prompted
3. Select **File Transfer** mode

#### 4ï¸âƒ£ Verify Connection

```bash
# Check if device is connected
adb devices
```

You should see output like:
```
List of devices attached
ABC123XYZ    device
```

#### 5ï¸âƒ£ First Build (Takes 5-10 minutes)

```bash
# Build native Android app and install on phone
npx expo run:android
```

This command will:
- âœ… Build the native Android project
- âœ… Install the app on your phone
- âœ… Start Metro bundler
- âœ… Launch the app automatically

**Wait for the app to open on your phone!**

#### 6ï¸âƒ£ Daily Development (After First Build)

For subsequent development sessions, you only need:

```bash
# Start Metro bundler
npx expo start
```

Then:
- **Automatic**: App auto-launches if phone is connected
- **Manual**: Scan QR code with Expo Go app
- **Fast Refresh**: Code changes reload automatically

---

### ğŸ“± Using the App

1. **App Opens** â†’ Hear "Welcome to SenseBridge..."

2. **Choose Mode:**
   - Say **"Blind Mode"** or tap the button
   - Say **"Sign Mode"** or tap the button
   - Say **"Deaf Mode"** or tap the button

3. **Grant Permissions:**
   - Camera permission (for Blind & Sign modes)
   - Microphone permission (for Deaf mode)

4. **Use Features:**
   - **Blind Mode**: Point camera at objects, tap "Check Currency" for notes
   - **Sign Mode**: Show hand signs to front camera
   - **Deaf Mode**: Tap "Start Listening", then speak

5. **Exit**: Tap "Exit" button or say "Exit"

---

## ğŸ“ Project Structure

```
SenseBridge/
â”œâ”€â”€ ğŸ“± App.js                    # Main app entry, navigation setup
â”‚
â”œâ”€â”€ ğŸ–¼ï¸ screens/                  # All app screens
â”‚   â”œâ”€â”€ WelcomeScreen.js         # Voice-controlled mode selection
â”‚   â”œâ”€â”€ BlindModeScreen.js       # Obstacle & currency detection
â”‚   â”œâ”€â”€ SignModeScreen.js        # Sign language recognition
â”‚   â””â”€â”€ DeafModeScreen.js        # Speech to sign avatar
â”‚
â”œâ”€â”€ ğŸ§© components/               # Reusable UI components
â”‚   â”œâ”€â”€ LargeButton.js           # Accessible button with voice feedback
â”‚   â”œâ”€â”€ VoiceFeedback.js         # Listening animation component
â”‚   â””â”€â”€ ModeIndicator.js         # Mode banner component
â”‚
â”œâ”€â”€ âš™ï¸ services/                 # Core business logic
â”‚   â”œâ”€â”€ storageService.js        # SQLite + AsyncStorage wrapper
â”‚   â”œâ”€â”€ voiceEngine.js           # TTS + STT integration
â”‚   â””â”€â”€ decisionEngine.js        # Alert throttling & normalization
â”‚
â”œâ”€â”€ ğŸ”§ modules/                  # Native Android modules (to implement)
â”‚   â”œâ”€â”€ TFLiteModule/            # TensorFlow Lite integration
â”‚   â”œâ”€â”€ VoskModule/              # Vosk speech recognition
â”‚   â”œâ”€â”€ MediaPipeModule/         # Hand landmark detection
â”‚   â””â”€â”€ UnityBridge/             # Unity 3D avatar control
â”‚
â”œâ”€â”€ ğŸ“¦ assets/                   # Models and resources
â”‚   â””â”€â”€ models/                  # AI models (.tflite files)
â”‚
â”œâ”€â”€ ğŸ“„ app.json                  # Expo configuration
â”œâ”€â”€ ğŸ“„ package.json              # Dependencies
â”œâ”€â”€ ğŸ“– README.md                 # This file
â”œâ”€â”€ ğŸ“– TESTING.md                # Testing guide
â””â”€â”€ ğŸ“– NATIVE_MODULES.md         # Native module implementation guide
```

---

## ğŸ¯ Current Implementation Status

### âœ… Fully Implemented (Ready to Test)

- âœ”ï¸ **React Native App Structure** - Complete navigation system
- âœ”ï¸ **All UI Screens** - Welcome, Blind, Sign, Deaf modes
- âœ”ï¸ **Voice Engine** - Text-to-speech working
- âœ”ï¸ **Storage Service** - SQLite database for settings
- âœ”ï¸ **Decision Engine** - Alert throttling and management
- âœ”ï¸ **Camera Integration** - Front & back camera previews
- âœ”ï¸ **Accessible UI** - Large buttons, haptics, voice feedback
- âœ”ï¸ **Voice Commands** - Mode selection and navigation

**You can test all UI features NOW!**

### ğŸš§ Pending (Requires Native Module Development)

- â³ **Obstacle Detection** - Needs TFLite module (mock data currently)
- â³ **Currency Recognition** - Needs TFLite module (mock data currently)
- â³ **Sign Recognition** - Needs MediaPipe + TFLite modules
- â³ **Speech-to-Text** - Needs Vosk module (placeholder currently)
- â³ **3D Avatar** - Needs Unity integration (emoji placeholder currently)

**See `NATIVE_MODULES.md` for implementation guide.**

---

## ğŸ› ï¸ Technologies Used

### Frontend
- **React Native** - Cross-platform mobile framework
- **Expo SDK 54** - Development platform
- **React Navigation** - Screen routing

### Services & APIs
- **Expo Speech** - Text-to-speech (Android TTS)
- **Expo Camera** - Camera access
- **Expo SQLite** - Local database
- **Expo Haptics** - Vibration feedback

### AI & ML (To be integrated)
- **TensorFlow Lite** - On-device AI inference
- **YOLOv5 Nano** - Object detection model
- **MediaPipe Hands** - Hand landmark detection
- **Vosk** - Offline speech recognition
- **Unity 2022 LTS** - 3D avatar rendering

### Data & Training
- **COCO Dataset** - Obstacle detection training
- **Kaggle Indian Currency** - Currency recognition
- **WLASL/ISL Datasets** - Sign language training

---

## ğŸ¤ Voice Commands Reference

| Command | Action | Available In |
|---------|--------|--------------|
| "Blind Mode" | Navigate to Blind Mode | Welcome Screen |
| "Sign Mode" | Navigate to Sign Mode | Welcome Screen |
| "Deaf Mode" | Navigate to Deaf Mode | Welcome Screen |
| "Check Currency" | Scan currency note | Blind Mode |
| "Exit" or "Back" | Return to Welcome Screen | Any Mode |
| "Help" | Show help information | Any Screen |

---

## ğŸ§ª Testing & Debugging

### View Console Logs
```bash
npx expo start
```
All `console.log()` outputs appear in the terminal.

### Clear Cache and Rebuild
```bash
cd android
./gradlew clean
cd ..
npx expo prebuild --clean
npx expo run:android
```

### Check Detailed Android Logs
```bash
adb logcat | findstr "ReactNativeJS"
```

### Common Issues

**Issue:** App won't build
- **Solution**: Clear cache and rebuild (see above)

**Issue:** Camera not working
- **Solution**: Check Settings â†’ Apps â†’ SenseBridge â†’ Permissions

**Issue:** No voice output
- **Solution**: Increase phone volume, check TTS is enabled

**Issue:** Device not detected
- **Solution**: Reinstall device drivers, try different USB cable

---

## ğŸ“Š Performance Expectations

- **App Size**: ~80-100MB (with models)
- **Memory Usage**: <200MB RAM
- **Battery Impact**: Moderate (camera processing)
- **Frame Rate**: 2-3 FPS (obstacle detection)
- **Response Time**: <500ms (voice feedback)

---

## ğŸ—ºï¸ Roadmap

### Phase 1: Foundation âœ… (Completed)
- Project setup
- UI components
- Navigation system
- Core services

### Phase 2: Native Integration ğŸ”¨ (In Progress)
- TFLite module
- Vosk module
- MediaPipe module
- Unity bridge

### Phase 3: AI Models ğŸ“‹ (Planned)
- Train currency model
- Train sign classifier
- Optimize models
- Test accuracy

### Phase 4: Polish & Release ğŸ¯ (Future)
- Performance optimization
- Battery efficiency
- Comprehensive testing
- App store deployment

---

## ğŸ‘¥ Contributing

Contributions are welcome! This is an accessibility-focused project aimed at helping people with disabilities.

**Areas for Contribution:**
- AI model training and optimization
- Native module development
- UI/UX improvements
- Testing and bug reports
- Documentation
- Translation to other languages

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see LICENSE file for details.

---

## ğŸ™ Acknowledgments

- **Google MediaPipe** - Hand tracking technology
- **Alpha Cephei (Vosk)** - Offline speech recognition
- **Google TensorFlow** - On-device AI framework
- **Ultralytics YOLOv5** - Object detection model
- **Expo Team** - React Native development platform
- **Open-source community** - For accessible technologies

---

## ğŸ“ Support

For issues, questions, or suggestions:
1. Check `TESTING.md` for common problems
2. Review `NATIVE_MODULES.md` for implementation details
3. Open an issue on the project repository

---

**Built with â¤ï¸ for accessibility and inclusion** ğŸŒ
