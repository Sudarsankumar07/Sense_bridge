# ğŸš€ SenseBridge -- Full Design & Implementation Document

------------------------------------------------------------------------

## ğŸ§­ Project Goal

SenseBridge is an **offline-first accessibility mobile application**
that enables communication between: - Deaf users - Mute users - Blind
users - Normal users

The system uses **on-device AI models** and **zero-cost open-source
technologies**.

------------------------------------------------------------------------

# ğŸŒŸ Core Features

## ğŸ¤Ÿ Offline Sign Language Recognition

Camera detects sign â†’ Converts to text â†’ Converts to voice output

## ğŸ§‘â€ğŸ¦» Sign Avatar for Deaf Users

Speech/Text â†’ Converted â†’ Avatar shows sign animation

## ğŸ‘¨â€ğŸ¦¯ Blind Assist Mode

-   Voice-based navigation
-   Obstacle detection
-   Currency detection (Notes + Coins)

## ğŸ—£ Voice Mode Selection (App Opening)

App speaks and listens offline for mode selection.

------------------------------------------------------------------------

# ğŸ§° Final Tech Stack

## ğŸ“± Mobile

-   React Native (CLI or Expo Bare)
-   TypeScript

## ğŸ¤– On-Device AI

-   MediaPipe Hands â†’ Hand tracking
-   TensorFlow Lite â†’ Sign classification
-   Vosk â†’ Offline speech recognition
-   YOLOv5 Nano TFLite â†’ Object detection
-   Custom YOLO TFLite â†’ Currency detection
-   Unity â†’ Avatar animation engine
-   Android Offline TTS â†’ Voice output

## ğŸ’¾ Storage

-   SQLite
-   Async Storage

## âŒ Backend

Not required for MVP.

------------------------------------------------------------------------

# ğŸ§  System Architecture

    Camera + Mic
          â†“
    On Device AI Models
          â†“
    Decision Engine
          â†“
    Output (Voice + Avatar + Text)

------------------------------------------------------------------------

# ğŸ“± App Opening Flow (Voice First UX)

1.  App opens
2.  Offline TTS speaks welcome message
3.  Offline STT listens for command
4.  Mode selected automatically

Commands: - Blind Mode - Sign Mode - Deaf Mode

------------------------------------------------------------------------

# ğŸ‘¨â€ğŸ¦¯ Blind Mode Implementation

## Obstacle Detection

Camera â†’ YOLO TFLite â†’ Distance Estimate â†’ Voice Alert

Alerts: - Obstacle ahead - Person in front - Step detected

## Currency Detection

Camera â†’ Currency YOLO â†’ Class Prediction â†’ TTS Output

------------------------------------------------------------------------

# ğŸ¤Ÿ Sign Recognition Implementation

Camera â†’ MediaPipe Hands â†’ TFLite Gesture Model â†’ Text â†’ TTS

Dataset: - Capture 500+ samples per sign - Train CNN â†’ Convert to TFLite

------------------------------------------------------------------------

# ğŸ§‘â€ğŸ¦» Deaf Mode Avatar Implementation

Speech â†’ Offline STT â†’ Text â†’ Word Mapping â†’ Avatar Animation

Avatar handled by Unity: - Prebuilt animations - Trigger animation based
on words

------------------------------------------------------------------------

# ğŸ¤ Voice Command Engine

Using Vosk Offline STT

Commands: - Blind Mode - Sign Mode - Deaf Mode - Exit - Repeat

------------------------------------------------------------------------

# ğŸ’¾ Dataset Strategy

## Sign Language

-   WLASL Dataset
-   Indian Sign Language Dataset
-   Self captured gestures

## Currency

-   Indian Currency Kaggle Dataset

## Object Detection

-   COCO Dataset subset

------------------------------------------------------------------------

# âš¡ Performance Planning

Minimum Device: - 4GB RAM - Android 10+

Approx AI Size: - Hand Model \~10MB - YOLO Model \~15MB - Currency Model
\~15MB - Vosk Model \~40MB

------------------------------------------------------------------------

# ğŸ¨ UI Design Guidelines

Theme: - Dark Mode - Neon Blue Accent - High Contrast Accessibility

Accessibility: - Big Buttons - Voice Feedback - Haptic Feedback

------------------------------------------------------------------------

# ğŸ›  Development Roadmap

## Phase 1 (Weeks 1-2)

-   Base App UI
-   Offline TTS
-   Voice Mode Selection

## Phase 2 (Weeks 3-5)

-   Sign Recognition Basic

## Phase 3 (Weeks 6-8)

-   Obstacle Detection

## Phase 4 (Weeks 9-10)

-   Currency Detection

## Phase 5 (Weeks 11-12)

-   Avatar Integration

------------------------------------------------------------------------

# ğŸ’° Cost

Total Cost = â‚¹0

All tools are open source and offline.

------------------------------------------------------------------------

# â­ Final Outcome

A futuristic offline accessibility mobile app using Edge AI and Computer
Vision that helps multiple disabled communities communicate and navigate
independently.
